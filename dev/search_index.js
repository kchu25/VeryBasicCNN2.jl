var documenterSearchIndex = {"docs":
[{"location":"#VeryBasicCNN2","page":"Home","title":"VeryBasicCNN2","text":"Documentation for VeryBasicCNN2.\n\n","category":"section"},{"location":"#VeryBasicCNN2.CodeProcessor","page":"Home","title":"VeryBasicCNN2.CodeProcessor","text":"CodeProcessor\n\nNetwork for processing concatenated code and gradient features.\n\nFields\n\nexpand_filters: Channel expansion (optional, for mbconv)\ndw_filters: Depthwise convolution filters\nse_w1, se_w2: Squeeze-excitation weights (optional, for mbconv)\nproject_filters: Channel projection back to output size\ndw_filters_2, project_filters_2: Second layer (deep_plain)\ndw_filters_3, project_filters_3: Third layer (deep_plain)\nmask_proj: Component-wise mask projection\nchannel_mask_proj: Channel-wise mask projection\nmask_temp, mask_eta, mask_gamma: Gumbel-Softmax parameters\nuse_hard_mask: Whether to use hard mask\nuse_residual: Whether to use skip connection\narch_type: Architecture type\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.CodeProcessor-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.CodeProcessor","text":"(cp::CodeProcessor)(x; training::Bool=true, step::Union{Nothing, Int}=nothing)\n\nForward pass through code processor - dispatches to architecture-specific implementation.\n\nProcess\n\nExtract identity for residual (if needed)\nForward through architecture-specific layers\nAdd residual connection (if enabled)\nApply hard mask (if enabled)\n\nArguments\n\nx: Input tensor (spatial, channels, 1, batch)\ntraining: Whether in training mode (affects Gumbel sampling)\nstep: Training step for temperature annealing\n\nReturns\n\nProcessed features\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.CodeProcessor-Tuple{}","page":"Home","title":"VeryBasicCNN2.CodeProcessor","text":"CodeProcessor(; kwargs...)\n\nConstruct a CodeProcessor with specified architecture.\n\nArguments\n\nin_channels::Int: Input channels (code + gradient concatenated)\nout_channels::Int: Output channels (same as code)\nkernel_size::Int=3: Depthwise conv kernel size\nexpansion_ratio::Int=2: Channel expansion for mbconv\nse_ratio::Float=8.0: SE reduction ratio for mbconv\nuse_se::Bool=true: Enable SE attention for mbconv\nuse_hard_mask::Bool=false: Enable Gumbel-Softmax masking\nmask_temp::Float=0.5: Initial temperature for Gumbel-Softmax\nmask_eta::Float=1.0: Right stretch parameter\nmask_gamma::Float=0.0: Left stretch parameter\narch_type::CodeProcessorType=mbconv: Architecture type\nuse_cuda::Bool=false: Use GPU\nrng: Random number generator\n\nReturns\n\nCodeProcessor instance\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.CodeProcessorType","page":"Home","title":"VeryBasicCNN2.CodeProcessorType","text":"CodeProcessorType\n\nType of architecture for code processing.\n\n:plain: Simple depthwise convolution\n:resnet: Depthwise conv with residual connection\n:mbconv: MBConv-style with expansion and SE attention\n:deep_plain: Stacked plain layers (3 layers for more capacity)\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.HyperParamRanges","page":"Home","title":"VeryBasicCNN2.HyperParamRanges","text":"HyperParamRanges\n\nSpecification of valid ranges for random hyperparameter generation.\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.HyperParameters","page":"Home","title":"VeryBasicCNN2.HyperParameters","text":"HyperParameters\n\nCNN hyperparameters for biological sequence analysis.\n\nArchitecture Fields\n\npfm_len::Int: Length of Position Weight Matrix filters (motif length)\nnum_pfms::Int: Number of PWM filters in base layer\nnum_img_filters::Vector{Int}: Number of filters per convolutional layer\nimg_fil_widths::Vector{Int}: Input channels for each conv layer\nimg_fil_heights::Vector{Int}: Filter heights for each conv layer\n\nPooling Fields\n\npool_base::Int: Pooling size for base layer\nstride_base::Int: Stride for base layer pooling\npoolsize::Vector{Int}: Pooling sizes for each conv layer\nstride::Vector{Int}: Strides for each conv layer\npool_lvl_top::Int: Highest layer index that uses pooling\n\nTraining Fields\n\nsoftmax_strength_img_fil::Float32: Softmax strength for filter normalization\nbatch_size::Int: Training batch size\ninference_code_layer::Int: Layer to extract code from (0 = PWM layer)\n\nNormalization Fields\n\nuse_layernorm::Bool: Apply LayerNorm after pooling for layers > inferencecodelayer (default: false)\nuse_channel_mask::Bool: Apply channel masking to PWM and conv layers (default: true)\n\nMBConv Fields\n\nnum_mbconv::Int: Number of MBConv blocks to add (0 = none, default)\nmbconv_expansion::Int: MBConv expansion ratio (default: 4)\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.LearnedCodeImgFilters","page":"Home","title":"VeryBasicCNN2.LearnedCodeImgFilters","text":"LearnedCodeImgFilters\n\nLearnable convolutional filters for intermediate CNN layers.\n\nFields\n\nfilters: 4D array (height, width, 1, num_filters)\nln_gamma: LayerNorm scale parameter (optional, per channel)\nln_beta: LayerNorm shift parameter (optional, per channel)\nmask: Channel masking layer (optional)\n\nForward Pass\n\nApplies normalized convolution followed by ReLU activation, optionally with LayerNorm and channel masking.\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.LearnedCodeImgFilters-Tuple{Any, HyperParameters}","page":"Home","title":"VeryBasicCNN2.LearnedCodeImgFilters","text":"(conv_filters::LearnedCodeImgFilters)(code_input, hp::HyperParameters; use_sparsity=false, training::Bool=true)\n\nForward pass through convolutional layer.\n\nProcess\n\nL2-normalize filters (optionally with sparsity)\nConvolve with input code\nApply ReLU activation\nOptional channel masking (if enabled)\n\nArguments\n\ncode_input: Input features\nhp: Hyperparameters\nuse_sparsity: Whether to apply sparsity weighting\ntraining: Whether in training mode (affects masking)\n\nReturns\n\nActivated code (4D: height, width, filters, batch)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.LearnedPWMs","page":"Home","title":"VeryBasicCNN2.LearnedPWMs","text":"LearnedPWMs\n\nLearnable Position Weight Matrices for the first CNN layer.\n\nFields\n\nfilters: 4D array (alphabetsize, motiflen, 1, num_filters)\nactivation_scaler: Scalar activation parameter\nmask: Channel masking layer (optional)\n\nForward Pass\n\nApplies PWM convolution followed by ReLU activation scaled by learned parameter, optionally with channel masking.\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.LearnedPWMs-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.LearnedPWMs","text":"(pwms::LearnedPWMs)(sequences; reverse_comp=false, training::Bool=true)\n\nForward pass through PWM layer.\n\nProcess\n\nCreate PWM from learned frequencies\nConvolve with input sequences  \nApply ReLU with learned scaling\nOptional channel masking (if enabled)\n\nArguments\n\nsequences: Input sequences\nreverse_comp: Whether to use reverse complement\ntraining: Whether in training mode (affects masking)\n\nReturns\n\nCode activations (3D: length, filters, batch)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.MBConvBlock","page":"Home","title":"VeryBasicCNN2.MBConvBlock","text":"MBConvBlock\n\nMobile Inverted Bottleneck Convolution (MBConv) block with SE attention.\n\nFields\n\nexpand_filters: Expansion filters (1, in_channels, 1, expanded)\ndw_filters: Depthwise filters (kernel_size, 1, 1, expanded)\nse_w1: SE reduction weights (se_channels, expanded, 1)\nse_w2: SE expansion weights (expanded, se_channels, 1)\nproject_filters: Projection filters (1, expanded, 1, out_channels)\nuse_skip: Whether to use skip connection\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.SeqCNN","page":"Home","title":"VeryBasicCNN2.SeqCNN","text":"SeqCNN\n\nConvolutional neural network for biological sequence analysis.\n\nArchitecture\n\nBase Layer: Learnable PWMs for motif detection\nConv Layers: Hierarchical feature extraction with pooling\nMBConv Blocks (optional): EfficientNet-style refinement\nOutput Layer: Linear transformation to predictions\n\nFields\n\nhp::HyperParameters: Architecture configuration\npwms::LearnedPWMs: Base layer PWM filters\nconv_layers::Vector{LearnedCodeImgFilters}: Convolutional layers\nmbconv_blocks::Vector{MBConvBlock}: Optional MBConv refinement blocks\noutput_weights::Array{Float32,3}: Final linear layer (outputdim × embeddim × 1)\n\nConstructor\n\nSeqCNN(hp, input_dims, output_dim; init_scale=0.5, use_cuda=true, rng=Random.GLOBAL_RNG)\n\nArguments\n\nhp: HyperParameters specifying architecture\ninput_dims: Tuple (alphabetsize, sequencelength)\noutput_dim: Number of output targets\ninit_scale: Weight initialization scale\nuse_cuda: Whether to place on GPU\nrng: Random number generator\n\nExample\n\nhp = generate_random_hyperparameters()\nmodel = SeqCNN(hp, (4, 41), 244)  # DNA sequences, 244 outputs\npredictions = model(sequences)\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.SeqCNN-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.SeqCNN","text":"(model::SeqCNN)(sequences; use_sparsity=false, linear_sum=false, predict_position=nothing)\n\nCallable interface for SeqCNN forward pass.\n\nArguments\n\nsequences: Input sequences\nuse_sparsity: Apply sparsity-inducing normalization\nlinear_sum: Return sum of linear outputs (for optimization)\npredict_position: Predict specific output position\n\nReturns\n\nModel predictions\n\nExamples\n\n# Standard prediction\npreds = model(sequences)\n\n# Linear sum for gradient-based optimization\nloss_term = model(sequences; linear_sum=true, predict_position=1)\n\n# Sparse filters\npreds_sparse = model(sequences; use_sparsity=true)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.layer_channel_mask","page":"Home","title":"VeryBasicCNN2.layer_channel_mask","text":"layer_channel_mask\n\nLearnable channel-wise masking layer using Gumbel-Softmax.\n\nFields\n\nmixing_filter: 1×C×1×C conv filter for channel mixing\ntemp: Gumbel-Softmax temperature\neta: Right stretch parameter for hard concrete\ngamma: Left stretch parameter for hard concrete\n\nProcess\n\nApply 1×1 conv for channel mixing\nSum over spatial dimension to get channel importance\nApply Gumbel-Softmax masking (soft in training, hard in test)\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN2.layer_channel_mask-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.layer_channel_mask","text":"(l::layer_channel_mask)(code; training=true)\n\nForward pass through channel masking layer.\n\nArguments\n\ncode: Input features, either (1, length, channels, batch) or (length, channels, 1, batch)\ntraining: Whether in training mode (soft masks) or test mode (hard masks)\n\nReturns\n\nMasked features with same shape as input\n\n\n\n\n\n","category":"method"},{"location":"#Base.getproperty-Tuple{SeqCNN, Symbol}","page":"Home","title":"Base.getproperty","text":"Custom property access for convenient model introspection and utilities.\n\nVirtual Properties\n\nmodel.num_conv_layers: Number of convolutional layers\nmodel.receptive_field: Receptive field at inference layer\nmodel.code: Function to extract code at inference layer\nmodel.first_layer_code: Function to extract base PWM code\nmodel.linear_sum: Function for linear output sum (optimization)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.amino_acid_ranges-Tuple{}","page":"Home","title":"VeryBasicCNN2.amino_acid_ranges","text":"amino_acid_ranges(; kwargs...)\n\nHyperparameter ranges optimized for amino acid sequences (proteins). 20-letter alphabet, larger filters needed.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.apply_gumbel_mask-Tuple{CodeProcessor, Any}","page":"Home","title":"VeryBasicCNN2.apply_gumbel_mask","text":"apply_gumbel_mask(cp::CodeProcessor, x; training::Bool=true, step::Union{Nothing, Int}=nothing)\n\nApply hierarchical Gumbel-Softmax masking (component-wise + channel-wise).\n\nArguments\n\ncp: CodeProcessor instance\nx: Input features (l, out_channels, 1, n)\ntraining: Whether in training mode (affects Gumbel sampling)\nstep: Training step for temperature annealing\n\nReturns\n\nMasked features with same shape as input\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.batched_mul-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN2.batched_mul","text":"batched_mul(A, B)\n\nBatched matrix multiplication wrapper for Flux.NNlib.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.clamp_positive-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.clamp_positive","text":"clamp_positive(x; upper=25)\n\nReLU with upper bound: min(upper, max(0, x))\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.compute_code_at_layer-Tuple{SeqCNN, Any, Int64}","page":"Home","title":"VeryBasicCNN2.compute_code_at_layer","text":"compute_code_at_layer(model::SeqCNN, sequences, layer; use_sparsity=false)\n\nCompute code representation at specified layer depth.\n\nArguments\n\nmodel: SeqCNN instance\nsequences: Input sequences (4D tensor)\nlayer: Target layer (0 = base PWM, 1+ = conv layers)\nuse_sparsity: Apply sparsity-inducing normalization\n\nReturns\n\nCode tensor at specified layer\n\nExamples\n\ncode_pwm = compute_code_at_layer(model, seqs, 0)      # Base PWM code\ncode_l1 = compute_code_at_layer(model, seqs, 1)       # After 1st conv\ncode_final = compute_code_at_layer(model, seqs, 3)    # After 3rd conv\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.compute_training_loss-Tuple{SeqCNN, Any, Any}","page":"Home","title":"VeryBasicCNN2.compute_training_loss","text":"compute_training_loss(model::SeqCNN, sequences, targets; use_sparsity=false, verbose=true)\n\nCompute training loss for the CNN model.\n\nArguments\n\nmodel: SeqCNN instance\nsequences: Input biological sequences\ntargets: Ground truth target values\nuse_sparsity: Apply sparsity-inducing normalization\nverbose: Print loss value\n\nReturns\n\nScalar loss value for optimization\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.conv-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN2.conv","text":"conv(x, w; pad=0, flipped=true)\n\nConvolution operation wrapper.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.conv_output_length-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN2.conv_output_length","text":"conv_output_length(input_len, filter_len)\n\nOutput length after 1D convolution: input_len - filter_len + 1\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.conv_pool_output_length-NTuple{4, Any}","page":"Home","title":"VeryBasicCNN2.conv_pool_output_length","text":"conv_pool_output_length(input_len, filter_len, pool_size, stride)\n\nOutput length after convolution followed by pooling.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.count_parameters-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.count_parameters","text":"count_parameters(model::SeqCNN)\n\nCount total trainable parameters in the model.\n\nReturns\n\nTotal number of trainable parameters\n\nExample\n\nmodel = SeqCNN(hp, (4, 41), 244)\nn_params = count_parameters(model)\nprintln(\"Model has $n_params trainable parameters\")\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.create_code_processor-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.create_code_processor","text":"create_code_processor(hp; kwargs...)\n\nCreate a CodeProcessor network based on hyperparameters.\n\nArguments\n\nhp: HyperParameters (determines input/output dimensions)\narch_type::CodeProcessorType=mbconv: Architecture type\nkernel_size::Int=3: Depthwise convolution kernel size\nexpansion_ratio::Int=2: Channel expansion ratio (mbconv only)\nuse_se::Bool=true: Enable SE attention (mbconv only)\nuse_hard_mask::Bool=false: Enable Gumbel-Softmax masking\nmask_temp::Float=0.5: Initial temperature\nmask_eta::Float=1.0: Right stretch parameter\nmask_gamma::Float=0.0: Left stretch parameter\nuse_cuda::Bool=true: Use GPU\nrng: Random number generator\n\nReturns\n\nCodeProcessor instance\n\nExample\n\nhp = generate_random_hyperparameters()\n\n# Plain depthwise conv\nproc_plain = create_code_processor(hp; arch_type=plain)\n\n# ResNet-style with residual\nproc_resnet = create_code_processor(hp; arch_type=resnet)\n\n# MBConv-style with SE attention\nproc_mbconv = create_code_processor(hp; arch_type=mbconv, expansion_ratio=4)\n\n# Forward pass: concatenate code and gradient\n# code_and_grad = cat(code, gradient; dims=2)\n# output = proc(code_and_grad)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.create_model-Tuple{Any, Any, Int64}","page":"Home","title":"VeryBasicCNN2.create_model","text":"create_model(input_dims, output_dim, batch_size; rng=Random.GLOBAL_RNG, use_cuda=true, ranges=DEFAULT_RANGES)\n\nCreate a SeqCNN with randomly generated hyperparameters.\n\nArguments\n\ninput_dims: Tuple (alphabetsize, sequencelength)\noutput_dim: Number of output targets\nbatch_size: Training batch size\nrng: Random number generator\nuse_cuda: Whether to use GPU\nranges: HyperParamRanges for architecture sampling\n\nReturns\n\nSeqCNN instance, or nothing if architecture is invalid\n\nExamples\n\n# DNA sequences\nmodel = create_model((4, 41), 244, 128; ranges=nucleotide_ranges())\n\n# Protein sequences\nmodel = create_model((20, 100), 1, 64; ranges=amino_acid_ranges())\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.create_pwm-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.create_pwm","text":"create_pwm(frequencies; reverse_comp=false)\n\nCreate Position Weight Matrix from frequency matrix. Converts to log2 odds ratios relative to background.\n\nPWM[i,j] = log2(freq[i,j] / background[i])\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.efficientnet_mbconv_config","page":"Home","title":"VeryBasicCNN2.efficientnet_mbconv_config","text":"efficientnet_mbconv_config(phi::Int=0)\n\nGet EfficientNet-style MBConv configuration based on compound scaling coefficient φ.\n\nEfficientNet Scaling\n\nφ=0 (B0): num_blocks=2, expansion=4  (baseline)\nφ=1 (B1): num_blocks=3, expansion=4  (1.2× depth)\nφ=2 (B2): num_blocks=3, expansion=6  (1.4× depth, wider)\nφ=3 (B3): num_blocks=4, expansion=6  (1.8× depth)\nφ=4 (B4): num_blocks=5, expansion=6  (2.2× depth)\n\nReturns\n\n(num_blocks, expansion): Configuration tuple\n\nExample\n\nhp = generate_random_hyperparameters()\nnum_blocks, expansion = efficientnet_mbconv_config(2)  # B2 config\nhp_b2 = with_mbconv(hp; num_blocks=num_blocks, expansion=expansion)\n\n\n\n\n\n","category":"function"},{"location":"#VeryBasicCNN2.eval!-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.eval!","text":"eval!(model::SeqCNN)\n\nSet model to evaluation mode (uses hard binary masks).\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.extract_features-Tuple{SeqCNN, Any}","page":"Home","title":"VeryBasicCNN2.extract_features","text":"extract_features(model::SeqCNN, sequences; use_sparsity=false)\n\nExtract CNN features from sequences (full forward pass through all conv layers).\n\nProcess\n\nBase PWM layer → pool\nAll conv layers → pool\nOptional MBConv refinement blocks\nFlatten to embedding vector\n\nReturns\n\nFeature embedding (embeddim, 1, batchsize)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.final_conv_embedding_length-Tuple{HyperParameters, Int64}","page":"Home","title":"VeryBasicCNN2.final_conv_embedding_length","text":"final_conv_embedding_length(hp::HyperParameters, seq_len::Int)\n\nCalculate the spatial dimension after all conv/pool layers. This simulates the full forward pass dimensionality.\n\nProcess\n\nBase layer: conv with PWM → pool\nEach layer ≤ poollvltop: conv → pool\nRemaining layers: conv only (no pool)\n\nReturns 0 if any dimension becomes invalid (≤ 0).\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.format_predictions-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.format_predictions","text":"format_predictions(linear_output)\n\nFormat linear output to appropriate prediction shape.\n\nReturns\n\n1D vector for single output\n2D matrix for multi-output (outputdim, batchsize)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.forward_conv_recursive","page":"Home","title":"VeryBasicCNN2.forward_conv_recursive","text":"forward_conv_recursive(model, code, current_layer, target_layer; use_sparsity=false, training=true)\n\nRecursively process convolutional layers.\n\nArguments\n\nmodel: SeqCNN instance\ncode: Current code representation\ncurrent_layer: Current layer index (1-indexed)\ntarget_layer: Stop at this layer (nothing = process all)\nuse_sparsity: Apply sparsity-inducing normalization\ntraining: Whether in training mode (affects channel masking)\n\nReturns\n\nCode after processing layers up to target_layer\n\n\n\n\n\n","category":"function"},{"location":"#VeryBasicCNN2.forward_deep_plain!-Tuple{CodeProcessor, Any}","page":"Home","title":"VeryBasicCNN2.forward_deep_plain!","text":"forward_deep_plain!(cp::CodeProcessor, x)\n\nDeep plain architecture with 3 stacked depthwise+projection layers, each followed by lightweight channel attention.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.forward_mbconv!-Tuple{CodeProcessor, Any}","page":"Home","title":"VeryBasicCNN2.forward_mbconv!","text":"forward_mbconv!(cp::CodeProcessor, x)\n\nMBConv-style with expansion, depthwise conv, optional SE attention, and projection.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.forward_plain!-Tuple{CodeProcessor, Any}","page":"Home","title":"VeryBasicCNN2.forward_plain!","text":"forward_plain!(cp::CodeProcessor, x)\n\nPlain depthwise convolution (no expansion, no SE, optional residual).\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.forward_resnet!-Tuple{CodeProcessor, Any}","page":"Home","title":"VeryBasicCNN2.forward_resnet!","text":"forward_resnet!(cp::CodeProcessor, x)\n\nResNet-style with depthwise conv + residual connection. Identical to plain but residual is handled externally.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.generate_random_hyperparameters-Tuple{}","page":"Home","title":"VeryBasicCNN2.generate_random_hyperparameters","text":"generate_random_hyperparameters(; batch_size=nothing, rng=Random.GLOBAL_RNG, ranges=DEFAULT_RANGES)\n\nGenerate randomized hyperparameters for architecture search.\n\nArguments\n\nbatch_size: Fixed batch size (defaults to random selection from ranges)\nrng: Random number generator for reproducibility\nranges: HyperParamRanges defining valid parameter ranges\n\nReturns\n\nHyperParameters instance with randomized valid configuration\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.gumbel_softmax_sample-NTuple{4, Any}","page":"Home","title":"VeryBasicCNN2.gumbel_softmax_sample","text":"gumbel_softmax_sample(p, temp, eta, gamma)\n\nSample from Gumbel-Softmax distribution for soft masking.\n\nArguments\n\np: Probabilities (any array type, CPU or GPU)\ntemp: Temperature (lower = sharper, higher = softer)\neta: Right stretch parameter for hard threshold\ngamma: Left stretch parameter for hard threshold\n\nReturns\n\nSoft mask values in [0, 1]\n\nExample\n\np = sigmoid.(randn(32, 1, 1))  # Channel probabilities\nz = gumbel_softmax_sample(p, 0.5, 1.0, 0.0)  # Soft masks\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.hard_threshold_mask-NTuple{4, Any}","page":"Home","title":"VeryBasicCNN2.hard_threshold_mask","text":"hard_threshold_mask(p, temp, eta, gamma)\n\nGenerate hard binary mask from probabilities (test time, no Gumbel noise).\n\nArguments\n\np: Probabilities\ntemp: Temperature for sharpening (typically 0.1 at test time)\neta: Right stretch parameter\ngamma: Left stretch parameter\n\nReturns\n\nHard binary mask (0.0 or 1.0)\n\nExample\n\np = sigmoid.(randn(32, 1, 1))\nz = hard_threshold_mask(p, 0.1, 1.0, 0.0)  # Binary: 0 or 1\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.huber_loss-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN2.huber_loss","text":"huber_loss(predictions, targets; delta=0.85, quadratic_weight=0.5)\n\nCompute Huber loss with automatic NaN handling.\n\nThe Huber loss combines quadratic loss for small errors and linear loss  for large errors, providing robustness to outliers.\n\nFormula\n\nFor |error| < δ: L = α * error²\nFor |error| ≥ δ: L = δ * (|error| - α * δ)\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth values\ndelta: Threshold between quadratic and linear regions\nquadratic_weight: Weight factor α for quadratic term\n\nReturns\n\nMean Huber loss over valid (non-NaN) entries\n\nNotes\n\nNaN values in targets are automatically excluded\nDifferentiable for gradient-based optimization\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.is_training-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.is_training","text":"is_training(model::SeqCNN)\n\nCheck if model is in training mode.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.layernorm-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN2.layernorm","text":"layernorm(x, gamma, beta; eps=DEFAULT_FLOAT_TYPE(1e-5))\n\nApply Layer Normalization.\n\nArguments\n\nx: Input tensor (height, width, channels, batch)\ngamma: Scale parameters (per channel)\nbeta: Shift parameters (per channel)\neps: Numerical stability constant\n\nReturns\n\nNormalized and affine-transformed tensor\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.masked_mse-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN2.masked_mse","text":"masked_mse(predictions, targets, mask)\n\nCompute MSE only on masked (valid) entries.\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth values\nmask: Boolean mask indicating valid entries\n\nReturns\n\nMean squared error over masked entries\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.maxpool-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.maxpool","text":"maxpool(x; pool_size=(2,1), stride=(1,1))\n\nApply 2D max pooling to 4D tensor (height, width, channels, batch).\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.model2cpu-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.model2cpu","text":"model2cpu(model::SeqCNN)\n\nConvert SeqCNN model from GPU to CPU.\n\nArguments\n\nmodel: SeqCNN instance (potentially on GPU)\n\nReturns\n\nSeqCNN instance with all arrays moved to CPU\n\nExample\n\nmodel_cpu = model2cpu(model)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.model2gpu-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.model2gpu","text":"model2gpu(model::SeqCNN)\n\nConvert SeqCNN model from CPU to GPU.\n\nArguments\n\nmodel: SeqCNN instance (potentially on CPU)\n\nReturns\n\nSeqCNN instance with all arrays moved to GPU\n\nExample\n\nmodel_gpu = model2gpu(model)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.normalize_filters_l2-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.normalize_filters_l2","text":"normalize_filters_l2(filters; softmax_alpha=SOFTMAX_ALPHA, use_sparsity=false)\n\nL2-normalize convolutional filters with optional sparsity-inducing weighting.\n\nArguments\n\nfilters: 4D filter tensor\nsoftmax_alpha: Strength of sparsity (higher = more sparse)\nuse_sparsity: Whether to apply softmax sparsity weighting\n\nReturns\n\nL2-normalized filters\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.normalize_squared-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.normalize_squared","text":"normalize_squared(matrix; ϵ=1e-5, reverse_comp=false)\n\nNormalize matrix by squaring elements and normalizing columns. Optionally concatenates reverse complement.\n\nProcess\n\nSquare all elements and add ϵ\nNormalize by column sums (creates probability distribution)\nOptionally create reverse complement and concatenate\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.nucleotide_ranges-Tuple{}","page":"Home","title":"VeryBasicCNN2.nucleotide_ranges","text":"nucleotide_ranges(; kwargs...)\n\nHyperparameter ranges optimized for nucleotide sequences (DNA/RNA). 4-letter alphabet, typical motif lengths 6-12nt.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.pool_code-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN2.pool_code","text":"pool_code(code, pool_size, stride; is_base_layer=false, skip_pooling=false)\n\nApply pooling to CNN code tensor with proper dimension handling.\n\nArguments\n\ncode: Input tensor (3D or 4D)\npool_size: Size as (height, width) tuple\nstride: Stride as (height, width) tuple  \nis_base_layer: Whether this is the base PWM layer (different indexing)\nskip_pooling: If true, only reshape without pooling (identity operation)\n\nReturns\n\nPooled 4D tensor (height, width, channels, batch)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.pool_output_length-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN2.pool_output_length","text":"pool_output_length(input_len, pool_size, stride)\n\nOutput length after pooling: (input_len - pool_size) ÷ stride + 1\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.predict_from_code-Tuple{SeqCNN, Any}","page":"Home","title":"VeryBasicCNN2.predict_from_code","text":"predict_from_code(model::SeqCNN, code; \n                 layer=0,\n                 use_sparsity=false,\n                 predict_position=nothing,\n                 apply_nonlinearity=true)\n\nMake predictions starting from code at any layer.\n\nArguments\n\nmodel: SeqCNN instance\ncode: Code representation at specified layer\nlayer: Which layer this code comes from (0 = PWM, 1+ = conv)\nuse_sparsity: Apply sparsity to remaining layers\npredict_position: Predict specific output\napply_nonlinearity: Apply final activation\n\nReturns\n\nPredictions\n\nExamples\n\n# Get code and predict from it\ncode = compute_code_at_layer(model, seqs, 2)\npreds = predict_from_code(model, code; layer=2)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.predict_from_sequences-Tuple{SeqCNN, Any}","page":"Home","title":"VeryBasicCNN2.predict_from_sequences","text":"predict_from_sequences(model::SeqCNN, sequences; \n                      use_sparsity=false, \n                      predict_position=nothing,\n                      apply_nonlinearity=true)\n\nComplete forward pass from sequences to predictions.\n\nProcess\n\nExtract CNN features\nLinear transformation (output layer)\nOptional nonlinearity (identity by default)\n\nArguments\n\nmodel: SeqCNN instance\nsequences: Input sequences\nuse_sparsity: Apply sparsity-inducing normalization\npredict_position: Predict specific output only\napply_nonlinearity: Apply final activation (currently identity)\n\nReturns\n\nPredictions (outputdim, batchsize) or (batch_size,) for single output\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.prepare_conv_params-Tuple{LearnedCodeImgFilters, HyperParameters}","page":"Home","title":"VeryBasicCNN2.prepare_conv_params","text":"prepare_conv_params(conv_filters::LearnedCodeImgFilters, hp::HyperParameters; use_sparsity=false)\n\nPrepare convolutional filter parameters.\n\nReturns\n\nL2-normalized filters, optionally with sparsity weighting\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.prepare_pwm_params-Tuple{LearnedPWMs}","page":"Home","title":"VeryBasicCNN2.prepare_pwm_params","text":"prepare_pwm_params(pwms::LearnedPWMs; reverse_comp=false)\n\nPrepare PWM parameters for forward pass.\n\nReturns\n\npwm_matrix: Position weight matrix (log odds ratios)\neta: Squared and clamped activation scaler\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.print_model_summary-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.print_model_summary","text":"print_model_summary(model::SeqCNN)\n\nPrint a summary of model architecture and parameter count.\n\nExample\n\nmodel = SeqCNN(hp, (4, 41), 244)\nprint_model_summary(model)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.process_code-Tuple{CodeProcessor, Any, Any}","page":"Home","title":"VeryBasicCNN2.process_code","text":"process_code(processor::CodeProcessor, code, gradient; training::Bool=true, step::Union{Nothing, Int}=nothing)\n\nProcess code and gradient features through the processor network.\n\nArguments\n\nprocessor: CodeProcessor instance\ncode: Code features at inference code layer (l, C, 1, n)\ngradient: Gradient features at same layer (l, C, 1, n)\ntraining: Whether in training mode\nstep: Current training step (for temperature annealing)\n\nReturns\n\nProcessed features (l, C, 1, n) - same size as code\n\nExample\n\n# In training loop\nfor (step, batch) in enumerate(dataloader)\n    output = process_code(proc, code, grad; training=true, step=step)\nend\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.processor2cpu-Tuple{CodeProcessor}","page":"Home","title":"VeryBasicCNN2.processor2cpu","text":"processor2cpu(proc::CodeProcessor)\n\nConvert CodeProcessor from GPU to CPU.\n\nArguments\n\nproc: CodeProcessor instance (potentially on GPU)\n\nReturns\n\nCodeProcessor instance with all arrays moved to CPU\n\nExample\n\nproc_cpu = processor2cpu(proc)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.processor2gpu-Tuple{CodeProcessor}","page":"Home","title":"VeryBasicCNN2.processor2gpu","text":"processor2gpu(proc::CodeProcessor)\n\nConvert CodeProcessor from CPU to GPU.\n\nArguments\n\nproc: CodeProcessor instance (potentially on CPU)\n\nReturns\n\nCodeProcessor instance with all arrays moved to GPU\n\nExample\n\nproc_gpu = processor2gpu(proc)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.receptive_field-Tuple{HyperParameters}","page":"Home","title":"VeryBasicCNN2.receptive_field","text":"receptive_field(hp::HyperParameters)\n\nCalculate receptive field size in the input sequence at the inference code layer. This accounts for all convolutions, pooling, and strides up to that layer.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.select_output_weights-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.select_output_weights","text":"select_output_weights(model::SeqCNN; predict_position=nothing)\n\nSelect output weights for prediction.\n\nArguments\n\nmodel: SeqCNN instance\npredict_position: Specific output index (nothing = all outputs)\n\nReturns\n\nOutput weight matrix or view\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.square_clamp-Tuple{Any}","page":"Home","title":"VeryBasicCNN2.square_clamp","text":"square_clamp(x)\n\nSquare and clamp to [0, 0.5] range.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.train!-Tuple{SeqCNN}","page":"Home","title":"VeryBasicCNN2.train!","text":"train!(model::SeqCNN)\n\nSet model to training mode (uses soft Gumbel-Softmax masks).\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.with_batch_size-Tuple{HyperParameters, Int64}","page":"Home","title":"VeryBasicCNN2.with_batch_size","text":"with_batch_size(hp::HyperParameters, new_batch_size::Int)\n\nCreate new HyperParameters with different batch size.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN2.with_efficientnet_mbconv","page":"Home","title":"VeryBasicCNN2.with_efficientnet_mbconv","text":"with_efficientnet_mbconv(hp::HyperParameters, phi::Int=0)\n\nAdd EfficientNet-style MBConv blocks with compound scaling coefficient φ.\n\nThis uses the standard EfficientNet scaling strategy where φ controls the depth (number of blocks) and width (expansion ratio) of MBConv layers.\n\nArguments\n\nhp: Base hyperparameters\nphi: EfficientNet scaling coefficient (0-7 for B0-B7)\n\nExample\n\nhp = generate_random_hyperparameters()\nhp_b0 = with_efficientnet_mbconv(hp, 0)  # EfficientNet-B0 style\nhp_b2 = with_efficientnet_mbconv(hp, 2)  # EfficientNet-B2 style\nhp_b4 = with_efficientnet_mbconv(hp, 4)  # EfficientNet-B4 style\n\n\n\n\n\n","category":"function"},{"location":"#VeryBasicCNN2.with_layernorm","page":"Home","title":"VeryBasicCNN2.with_layernorm","text":"with_layernorm(hp::HyperParameters, enabled::Bool=true)\n\nCreate new HyperParameters with LayerNorm enabled or disabled.\n\nLayerNorm is applied after pooling for layers > inferencecodelayer when enabled.\n\nExample\n\nhp = generate_random_hyperparameters()\nhp_ln = with_layernorm(hp, true)   # Enable LayerNorm\nhp_no_ln = with_layernorm(hp, false)  # Disable LayerNorm\n\n\n\n\n\n","category":"function"},{"location":"#VeryBasicCNN2.with_mbconv-Tuple{HyperParameters}","page":"Home","title":"VeryBasicCNN2.with_mbconv","text":"with_mbconv(hp::HyperParameters; num_blocks=2, expansion=4)\n\nCreate new HyperParameters with MBConv blocks enabled.\n\nExample\n\nhp = generate_random_hyperparameters()\nhp_mbconv = with_mbconv(hp; num_blocks=3, expansion=6)\n\n\n\n\n\n","category":"method"}]
}
